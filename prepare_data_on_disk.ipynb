{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "dict_keys(['person_images', 'ca_images', 'garment_images', 'person_poses', 'garment_poses'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from clothing_segmentation import HumanParser\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from torchvision import transforms as T\n",
    "\n",
    "\n",
    "\n",
    "class SyntheticTryonDatasetFromDisk(Dataset):\n",
    "    def __init__(self, path='/mnt/datadrive/asos_dataset/prepared_64/tensors/'):\n",
    "        self.path = Path(path)\n",
    "        self.glob=sorted(self.path.rglob('*.pt'), key=lambda x: int(x.stem))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.glob)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tensor_path = self.glob[idx]\n",
    "        record = torch.load(tensor_path)\n",
    "        return record\n",
    "    \n",
    "    \n",
    "xxxx = SyntheticTryonDatasetFromDisk()\n",
    "print(len(xxxx))\n",
    "for i in xxxx:\n",
    "    print(i.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:03,  1.19s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/roman/tryondiffusion_implementation/tryondiffusion_danny/prepare_data_on_disk.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btryon/home/roman/tryondiffusion_implementation/tryondiffusion_danny/prepare_data_on_disk.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m dataset \u001b[39m=\u001b[39m SyntheticTryonDataset(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btryon/home/roman/tryondiffusion_implementation/tryondiffusion_danny/prepare_data_on_disk.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m         num_samples\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, image_size\u001b[39m=\u001b[39mSR_UNET_IMAGE_SIZE \u001b[39mif\u001b[39;00m TRAIN_UNET_NUMBER \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m BASE_UNET_IMAGE_SIZE\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btryon/home/roman/tryondiffusion_implementation/tryondiffusion_danny/prepare_data_on_disk.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btryon/home/roman/tryondiffusion_implementation/tryondiffusion_danny/prepare_data_on_disk.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m prepared_ds_path \u001b[39m=\u001b[39m Path(\u001b[39m'\u001b[39m\u001b[39m/mnt/datadrive/asos_dataset/prepared_64/tensors\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btryon/home/roman/tryondiffusion_implementation/tryondiffusion_danny/prepare_data_on_disk.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, sample \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(dataset)):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btryon/home/roman/tryondiffusion_implementation/tryondiffusion_danny/prepare_data_on_disk.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     torch\u001b[39m.\u001b[39msave(sample, prepared_ds_path \u001b[39m/\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/venv_tryondiffusion_implementation/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/tryondiffusion_implementation/tryondiffusion_danny/tryondiffusion/dataset.py:93\u001b[0m, in \u001b[0;36mSyntheticTryonDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     91\u001b[0m np_person_image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(person_image)\n\u001b[1;32m     92\u001b[0m person_image_resized \u001b[39m=\u001b[39m person_image\u001b[39m.\u001b[39mresize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size, Image\u001b[39m.\u001b[39mBICUBIC)\n\u001b[0;32m---> 93\u001b[0m person_image_hp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhuman_parser\u001b[39m.\u001b[39;49mforward_img(person_image)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     95\u001b[0m ca_image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_clothing_agnostic(np_person_image, person_image_hp)\n\u001b[1;32m     96\u001b[0m person_pose \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_pose(person_image_resized)\n",
      "File \u001b[0;32m~/tryondiffusion_implementation/tryondiffusion_danny/clothing_segmentation/human_parsing.py:30\u001b[0m, in \u001b[0;36mHumanParser.forward_img\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     27\u001b[0m     logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits  \u001b[39m# shape (batch_size, num_labels, height/4, width/4)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m# print(f'{logits.shape=}')\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m# print(f'{image.size}')\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m upsampled_logits \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49minterpolate(\n\u001b[1;32m     31\u001b[0m     logits,\n\u001b[1;32m     32\u001b[0m     size\u001b[39m=\u001b[39;49mimage\u001b[39m.\u001b[39;49msize[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], \u001b[39m# (height, width)\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbilinear\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     34\u001b[0m     align_corners\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[39m# print(f'{upsampled_logits.shape=}')\u001b[39;00m\n\u001b[1;32m     38\u001b[0m pred_seg \u001b[39m=\u001b[39m upsampled_logits\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/venv_tryondiffusion_implementation/lib/python3.10/site-packages/torch/nn/functional.py:4038\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4032\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mare_deterministic_algorithms_enabled() \u001b[39mand\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mis_cuda:\n\u001b[1;32m   4033\u001b[0m             \u001b[39m# Use slow decomp whose backward will be in terms of index_put\u001b[39;00m\n\u001b[1;32m   4034\u001b[0m             \u001b[39m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[1;32m   4035\u001b[0m             \u001b[39m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[1;32m   4036\u001b[0m             \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m'\u001b[39m\u001b[39mtorch._decomp.decompositions\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mupsample_bilinear2d_vec(\n\u001b[1;32m   4037\u001b[0m                 \u001b[39minput\u001b[39m, output_size, align_corners, scale_factors)\n\u001b[0;32m-> 4038\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mupsample_bilinear2d(\u001b[39minput\u001b[39;49m, output_size, align_corners, scale_factors)\n\u001b[1;32m   4039\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrilinear\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   4040\u001b[0m     \u001b[39massert\u001b[39;00m align_corners \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append('/home/roman/tryondiffusion_implementation/tryondiffusion_danny/tryondiffusion')\n",
    "from tryondiffusion import TryOnImagen, TryOnImagenTrainer, get_unet_by_name, SyntheticTryonDataset, tryondiffusion_collate_fn\n",
    "\n",
    "\n",
    "TRAIN_UNET_NUMBER = 1\n",
    "BASE_UNET_IMAGE_SIZE = (64, 64)\n",
    "SR_UNET_IMAGE_SIZE = (256, 256)\n",
    "BATCH_SIZE = 2 \n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "NUM_ITERATIONS = 100\n",
    "TIMESTEPS = (1000, 1000)\n",
    "\n",
    "dataset = SyntheticTryonDataset(\n",
    "        num_samples=500, image_size=SR_UNET_IMAGE_SIZE if TRAIN_UNET_NUMBER == 2 else BASE_UNET_IMAGE_SIZE\n",
    "    )\n",
    "\n",
    "prepared_ds_path = Path('/mnt/datadrive/asos_dataset/prepared_64/tensors')\n",
    "\n",
    "for idx, sample in tqdm(enumerate(dataset)):\n",
    "    torch.save(sample, prepared_ds_path / f'{idx}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tryondiffusion_implementation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
