{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/datadrive/asos_dataset/prepared_64/tensors/0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/604 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from clothing_segmentation import HumanParser\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from torchvision import transforms as T\n",
    "\n",
    "\n",
    "\n",
    "class SyntheticTryonDatasetFromDisk(Dataset):\n",
    "    def __init__(self, path='/mnt/datadrive/asos_dataset/prepared_64/tensors/'):\n",
    "        self.path = Path(path)\n",
    "        self.glob=sorted(self.path.rglob('*.pt'), key=lambda x: int(x.stem))\n",
    "        print(self.glob[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.glob)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tensor_path = self.glob[idx]\n",
    "        record = torch.load(tensor_path)\n",
    "        return record\n",
    "\n",
    "\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "xxxx = SyntheticTryonDatasetFromDisk()\n",
    "bad_list=[]\n",
    "from tqdm import tqdm\n",
    "for idx, i in enumerate(tqdm(xxxx, total=len(xxxx))):\n",
    "    # print(i)\n",
    "    print(i['person_poses'].shape)\n",
    "    \n",
    "    \n",
    "    # print(i['garment_poses'])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def __call__(self, input_image, detect_resolution=512, include_hand=False, include_face=False):\n",
    "    \n",
    "#     if not isinstance(input_image, np.ndarray):\n",
    "#         input_image = np.array(input_image, dtype=np.uint8)\n",
    "\n",
    "#     input_image = HWC3(input_image)\n",
    "#     input_image = resize_image(input_image, detect_resolution)\n",
    "\n",
    "#     poses = self.detect_poses(input_image, include_hand, include_face)\n",
    "    \n",
    "#     xy=[]\n",
    "#     if len(poses)>0:\n",
    "#         for point in poses[0].body.keypoints:\n",
    "#             if point is not None:\n",
    "#                 xy.append([point.x, point.y])\n",
    "#             else:\n",
    "#                 xy.append([0, 0])\n",
    "#         return np.array(xy)\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "from controlnet_aux import OpenposeDetector\n",
    "from controlnet_aux.open_pose import HWC3, resize_image\n",
    "from diffusers.utils import load_image\n",
    "input_image = target_img\n",
    "if not isinstance(input_image, np.ndarray):\n",
    "    input_image = np.array(input_image, dtype=np.uint8)\n",
    "input_image = input_image.astype(np.uint8)\n",
    "\n",
    "input_image = HWC3(input_image)\n",
    "input_image = resize_image(input_image, 512)\n",
    "\n",
    "poses = dataset.openpose.detect_poses(input_image, include_hand=False, include_face=False)\n",
    "\n",
    "# xy=[]\n",
    "# if len(poses)>0:\n",
    "#     for point in poses[0].body.keypoints:\n",
    "#         if point is not None:\n",
    "#             xy.append([point.x, point.y])\n",
    "#         else:\n",
    "#             xy.append([0, 0])\n",
    "#     return np.array(xy)\n",
    "# else:\n",
    "#     return None\n",
    "poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.9882353 , 0.9882353 , 0.9882353 ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        ...,\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ]],\n",
       "\n",
       "       [[1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        ...,\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ]],\n",
       "\n",
       "       [[1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        ...,\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        ...,\n",
       "        [0.99215686, 0.99215686, 0.99215686],\n",
       "        [0.99215686, 0.99215686, 0.99215686],\n",
       "        [0.99215686, 0.99215686, 0.99215686]],\n",
       "\n",
       "       [[1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        ...,\n",
       "        [0.9882353 , 0.9882353 , 0.9882353 ],\n",
       "        [0.9882353 , 0.9882353 , 0.9882353 ],\n",
       "        [0.9882353 , 0.9882353 , 0.9882353 ]],\n",
       "\n",
       "       [[1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        ...,\n",
       "        [0.9764706 , 0.9764706 , 0.9764706 ],\n",
       "        [0.9764706 , 0.9764706 , 0.9764706 ],\n",
       "        [0.9764706 , 0.9764706 , 0.9764706 ]]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "target_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/mnt/datadrive/asos_dataset/tshirts_orig_bigest/65/2288.png',\n",
       "       '/mnt/datadrive/asos_dataset/tshirts_orig_bigest/65/2289.png',\n",
       "       '/mnt/datadrive/asos_dataset/tshirts_orig_bigest/65/2290.png'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.items_reverse_index[65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[65,\n",
       " 109,\n",
       " 128,\n",
       " 135,\n",
       " 136,\n",
       " 179,\n",
       " 212,\n",
       " 227,\n",
       " 232,\n",
       " 250,\n",
       " 269,\n",
       " 318,\n",
       " 361,\n",
       " 440,\n",
       " 455,\n",
       " 465,\n",
       " 466,\n",
       " 495,\n",
       " 531,\n",
       " 534,\n",
       " 535,\n",
       " 545,\n",
       " 568]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roman/venv_tryondiffusion_implementation/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/roman/venv_tryondiffusion_implementation/lib/python3.10/site-packages/controlnet_aux/mediapipe_face/mediapipe_face_common.py:7: UserWarning: The module 'mediapipe' is not installed. The package will have limited functionality. Please install it using the command: pip install 'mediapipe'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append('/home/roman/tryondiffusion_implementation/tryondiffusion_danny/tryondiffusion')\n",
    "from tryondiffusion import TryOnImagen, TryOnImagenTrainer, get_unet_by_name, SyntheticTryonDataset, tryondiffusion_collate_fn\n",
    "\n",
    "\n",
    "# TODO resize pose from 512 to 64\n",
    "\n",
    "\n",
    "TRAIN_UNET_NUMBER = 1\n",
    "BASE_UNET_IMAGE_SIZE = (64, 64)\n",
    "SR_UNET_IMAGE_SIZE = (256, 256)\n",
    "BATCH_SIZE = 2 \n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "NUM_ITERATIONS = 100\n",
    "TIMESTEPS = (1000, 1000)\n",
    "\n",
    "dataset = SyntheticTryonDataset(image_size=SR_UNET_IMAGE_SIZE if TRAIN_UNET_NUMBER == 2 else BASE_UNET_IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 604/604 [18:05<00:00,  1.80s/it]\n"
     ]
    }
   ],
   "source": [
    "prepared_ds_path = Path('/mnt/datadrive/asos_dataset/prepared_64/tensors')\n",
    "\n",
    "# for idx, sample in enumerate(tqdm(dataset, total=len(dataset))):\n",
    "#     if idx > 103:\n",
    "#         # print(sample.keys())\n",
    "#         # for k,v in sample.items():\n",
    "#         #     print(v.shape)\n",
    "#         try:\n",
    "#             torch.save(sample, prepared_ds_path / f'{idx}.pt')\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             print(idx)\n",
    "\n",
    "from tqdm import trange\n",
    "for idx in trange(0, len(dataset)):\n",
    "    try:\n",
    "        sample = dataset.__getitem__(idx)\n",
    "        torch.save(sample, prepared_ds_path / f'{idx}.pt')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tryondiffusion_implementation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
