{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natten import (\n",
    "  NeighborhoodAttention1D,\n",
    "  NeighborhoodAttention2D,\n",
    "  NeighborhoodAttention3D,\n",
    ")\n",
    "na1d = NeighborhoodAttention1D(dim=128, kernel_size=7, dilation=3, num_heads=4)\n",
    "na2d = NeighborhoodAttention2D(dim=128, kernel_size=7, dilation=3, num_heads=4)\n",
    "na3d = NeighborhoodAttention3D(dim=128, kernel_size=7, dilation=3, num_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'head_dim=32, num_heads=4, kernel_size=7, dilation=3, has_bias=True'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na1d.extra_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullpath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            fullpath\n",
       "0  /mnt/datadrive/asos_dataset/tshirts_orig_biges...\n",
       "1  /mnt/datadrive/asos_dataset/tshirts_orig_biges...\n",
       "2  /mnt/datadrive/asos_dataset/tshirts_orig_biges...\n",
       "3  /mnt/datadrive/asos_dataset/tshirts_orig_biges...\n",
       "4  /mnt/datadrive/asos_dataset/tshirts_orig_biges..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "datapath = '/mnt/datadrive'\n",
    "x = list(filter(lambda x: x.suffix=='.png', Path(datapath).rglob('*')))\n",
    "df = pd.DataFrame(x, columns=['fullpath'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item_idx\n",
       "0      4\n",
       "471    4\n",
       "465    4\n",
       "466    4\n",
       "467    4\n",
       "      ..\n",
       "276    4\n",
       "277    4\n",
       "278    4\n",
       "279    4\n",
       "99     4\n",
       "Name: count, Length: 609, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['item_idx'] = df['fullpath'].apply(lambda x: int(x.parent.stem))\n",
    "df['item_idx'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_imgs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# for j in [0, 1, 2, 3, 44, 55, 99]:\n",
    "#     for i in df[df['item_idx']==j]['fullpath'].values:\n",
    "#         print(i)\n",
    "#         img = Image.open(i)\n",
    "#         plt.imshow(img)\n",
    "#         plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "                                            fullpath  item_idx\n",
      "0  /mnt/datadrive/asos_dataset/tshirts_orig_biges...         0\n",
      "1  /mnt/datadrive/asos_dataset/tshirts_orig_biges...         0\n",
      "2  /mnt/datadrive/asos_dataset/tshirts_orig_biges...         0\n",
      "3  /mnt/datadrive/asos_dataset/tshirts_orig_biges...         0\n"
     ]
    }
   ],
   "source": [
    "for group_idx, group in df.groupby(by='item_idx'):\n",
    "    print(group_idx)\n",
    "    print(group)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open(image_path).convert(\"RGB\") \n",
    "image = Image.open('/mnt/datadrive/asos_dataset/tshirts_orig_bigest/9/2397.png').convert(\"RGB\")\n",
    "res = hp.forward_img(image).squeeze(0)\n",
    "\n",
    "def get_clothing_agnostic(image, hp_mask, classes_to_rm=[4,6]):\n",
    "    bg_color = (255,255,255)\n",
    "    assert image.shape[:-1] == hp_mask.shape\n",
    "    cloths_to_rm_mask = np.zeros(hp_mask.shape)\n",
    "    for i in np.unique(res):\n",
    "        if i in classes_to_rm:\n",
    "            cloths_to_rm_mask[res==i] = 255\n",
    "    image[cloths_to_rm_mask!=0] = bg_color\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2243, 1758)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 4 === upper\n",
    "# 6 === lower \n",
    "\n",
    "def get_clothing_agnostic(image, hp_mask, classes_to_rm=[4,6]):\n",
    "    bg_color = (255,255,255)\n",
    "    assert image.shape[:-1] == hp_mask.shape\n",
    "    cloths_to_rm_mask = np.zeros(hp_mask.shape)\n",
    "    for i in np.unique(res):\n",
    "        if i in classes_to_rm:\n",
    "            cloths_to_rm_mask[res==i] = 255\n",
    "    image[cloths_to_rm_mask!=0] = bg_color\n",
    "    return image\n",
    "\n",
    "# xx = np.zeros(res.shape)\n",
    "# for i in np.unique(res):\n",
    "#     if i in [4,6]:\n",
    "#         xx[res==i] = 255\n",
    "# np_image = np.array(image)\n",
    "# np_image[xx!=0] = (255,255,255)\n",
    "# plt.imshow(np_image)\n",
    "# plt.show()\n",
    "print(np_image.shape[:-1])\n",
    "# plt.imshow(xx)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['person_images', 'ca_images', 'garment_images', 'person_poses', 'garment_poses'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from clothing_segmentation import HumanParser\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision import transforms as T\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# class SyntheticTryonDataset(Dataset):\n",
    "    def __init__(self, image_size=(64,64), pose_size=(18, 2), apply_transform=True):\n",
    "\n",
    "        self.human_parser = HumanParser()\n",
    "        self.transform = T.Compose([\n",
    "            # T.Resize(image_size),\n",
    "            # T.CenterCrop(image_size),\n",
    "            T.ToTensor(),\n",
    "        ])\n",
    "        self.apply_transform = apply_transform\n",
    "        self.df = pd.read_csv('/home/roman/tryondiffusion_implementation/tryondiffusion_danny/all_imgs.csv')\n",
    "        self.df = self.df[self.df['pose_detected']]\n",
    "        \n",
    "        self.items_reverse_index = {}\n",
    "        for group_idx, group in self.df.groupby(by='item_idx'):\n",
    "            self.items_reverse_index[group_idx] = group['fullpath'].values        \n",
    "        \n",
    "        self.image_size = image_size\n",
    "        # self.pose_size = pose_size\n",
    "        self.openpose = MyOpenPoseDetector.from_pretrained(\"lllyasviel/ControlNet\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items_reverse_index)\n",
    "    \n",
    "    def prepare_clothing_agnostic(self, img, hp_mask)-> np.array: \n",
    "        # classes_to_rm=[4,6] \n",
    "        classes_to_rm=[4]      \n",
    "        # def get_clothing_agnostic(image, hp_mask, classes_to_rm=[4,6]):\n",
    "        bg_color = (255,255,255)\n",
    "        assert img.shape[:-1] == hp_mask.shape\n",
    "        # cloths_to_rm_mask = np.zeros(hp_mask.shape)\n",
    "        # for i in np.unique(res):\n",
    "        #     if i in classes_to_rm:\n",
    "        #         cloths_to_rm_mask[res==i] = 255\n",
    "        cloths_to_rm_mask = np.isin(hp_mask, classes_to_rm)\n",
    "        img[cloths_to_rm_mask!=0] = bg_color\n",
    "        return img\n",
    "        \n",
    "    def prepare_pose(self, img):\n",
    "        # pass\n",
    "        pose = self.openpose(img, detect_resolution=self.image_size[0])\n",
    "        return pose\n",
    "\n",
    "    def prepare_segmented_garment(self, img, hp_mask)-> np.array:\n",
    "        # classes_to_rm=[4,6] \n",
    "        classes_to_rm=[4]        \n",
    "        bg_color = (255,255,255)\n",
    "        assert img.shape[:-1] == hp_mask.shape\n",
    "        # cloths_to_rm_mask = np.zeros(hp_mask.shape)\n",
    "        # for i in np.unique(res):\n",
    "        #     if i in classes_to_rm:\n",
    "        #         cloths_to_rm_mask[res==i] = 255\n",
    "        cloths_to_rm_mask = np.isin(hp_mask, classes_to_rm)\n",
    "        img[cloths_to_rm_mask==0] = bg_color\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if idx in self.items_reverse_index:\n",
    "            items_images_list = self.items_reverse_index[idx]\n",
    "        else:\n",
    "            items_images_list = self.items_reverse_index[idx-1]\n",
    "        img_person = items_images_list[0]\n",
    "        img_garment = items_images_list[1]\n",
    "        \n",
    "        # inputs from img1\n",
    "        # noisy\n",
    "        # clothing agnostic\n",
    "        # person pose\n",
    "        # img = img.resize((768, 768), Image.BICUBIC)\n",
    "        person_image = Image.open(img_person).convert('RGB').resize((768, 768), Image.BICUBIC)\n",
    "        # print(person_image.size)\n",
    "        # .resize(self.image_size, Image.BICUBIC)\n",
    "        np_person_image = np.array(person_image)\n",
    "        person_image_resized = person_image.resize(self.image_size, Image.BICUBIC)\n",
    "        person_image_hp = self.human_parser.forward_img(person_image).squeeze(0)\n",
    "        \n",
    "        ca_image = self.prepare_clothing_agnostic(np_person_image, person_image_hp)\n",
    "        person_pose = self.prepare_pose(person_image_resized)\n",
    "        \n",
    "        # inputs from img2\n",
    "        # garment pose\n",
    "        # segmented garmend\n",
    "        garment_image = Image.open(img_garment).convert('RGB').resize((768, 768), Image.BICUBIC)\n",
    "        # print(garment_image.size)\n",
    "        # .resize(self.image_size, Image.BICUBIC)\n",
    "        np_garment_image = np.array(garment_image)\n",
    "        garment_image_hp = self.human_parser.forward_img(garment_image).squeeze(0)\n",
    "        \n",
    "        segmented_garment = self.prepare_segmented_garment(np_garment_image, garment_image_hp) \n",
    "        garment_pose = self.prepare_pose(garment_image.resize(self.image_size, Image.BICUBIC))\n",
    "        \n",
    "        # person_image = torch.randn(3, *self.image_size)\n",
    "        # ca_image = torch.randn(3, *self.image_size)\n",
    "        # garment_image = torch.randn(3, *self.image_size)\n",
    "        # person_pose = torch.randn(*self.pose_size)\n",
    "        # garment_pose = torch.randn(*self.pose_size)\n",
    "\n",
    "        # sample = {\n",
    "        #     \"person_images\": person_image,\n",
    "        #     \"ca_images\": ca_image,\n",
    "        #     \"garment_images\": garment_image,\n",
    "        #     \"person_poses\": person_pose,\n",
    "        #     \"garment_poses\": garment_pose,\n",
    "        # }\n",
    "\n",
    "        sample = {\n",
    "            \"person_images\": person_image_resized,\n",
    "            \"ca_images\": Image.fromarray(ca_image.astype('uint8')).resize(self.image_size, Image.BICUBIC),\n",
    "            \"garment_images\": Image.fromarray(segmented_garment.astype('uint8')).resize(self.image_size, Image.BICUBIC),\n",
    "            \"person_poses\": person_pose,\n",
    "            \"garment_poses\": garment_pose,\n",
    "        }\n",
    "        \n",
    "        # if self.apply_transform:\n",
    "        #     sample = {\n",
    "        #         \"person_images\": self.transform(sample['person_images']),\n",
    "        #         \"ca_images\": self.transform(sample['ca_images']),\n",
    "        #         \"garment_images\": self.transform(sample['garment_images']),\n",
    "        #         \"person_poses\": sample['person_poses'],\n",
    "        #         \"garment_poses\": sample['garment_poses']\n",
    "        #     }\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "\n",
    "\n",
    "def tryondiffusion_collate_fn(batch):\n",
    "    return {\n",
    "        \"person_images\": torch.stack([item[\"person_images\"] for item in batch]),\n",
    "        \"ca_images\": torch.stack([item[\"ca_images\"] for item in batch]),\n",
    "        \"garment_images\": torch.stack([item[\"garment_images\"] for item in batch]),\n",
    "        \"person_poses\": torch.stack([item[\"person_poses\"] for item in batch]),\n",
    "        \"garment_poses\": torch.stack([item[\"garment_poses\"] for item in batch]),\n",
    "    }\n",
    "\n",
    "ds = SyntheticTryonDataset()\n",
    "for i in ds:\n",
    "    print(i.keys())\n",
    "    plt.imshow(i['ca_images'])\n",
    "    plt.show()\n",
    "    break\n",
    "\n",
    "# train_dataloader = DataLoader(\n",
    "#     ds,\n",
    "#     batch_size=2,\n",
    "#     shuffle=True,\n",
    "#     collate_fn=tryondiffusion_collate_fn,\n",
    "# )\n",
    "\n",
    "# for i in train_dataloader:\n",
    "#     for k,v in i.items():\n",
    "#         print(k,v.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "609"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds.items_reverse_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullpath</th>\n",
       "      <th>item_idx</th>\n",
       "      <th>pose_detected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            fullpath  item_idx  pose_detected\n",
       "0  /mnt/datadrive/asos_dataset/tshirts_orig_biges...         0           True\n",
       "1  /mnt/datadrive/asos_dataset/tshirts_orig_biges...         0           True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/roman/tryondiffusion_implementation/tryondiffusion_danny/all_imgs.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 17104/20000 [3:06:22<30:56,  1.56it/s]  "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from controlnet_aux import OpenposeDetector\n",
    "from controlnet_aux.open_pose import HWC3, resize_image\n",
    "from diffusers.utils import load_image\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/roman/parse_zara/df_clean_clean_80756.csv')\n",
    "\n",
    "class MyOpenPoseDetector(OpenposeDetector):\n",
    "    \n",
    "    def __call__(self, input_image, detect_resolution=512, include_hand=False, include_face=False):\n",
    "       \n",
    "        if not isinstance(input_image, np.ndarray):\n",
    "            input_image = np.array(input_image, dtype=np.uint8)\n",
    "\n",
    "        input_image = HWC3(input_image)\n",
    "        input_image = resize_image(input_image, detect_resolution)\n",
    "\n",
    "        poses = self.detect_poses(input_image, include_hand, include_face)\n",
    "        xy=[]\n",
    "        if len(poses)>0:\n",
    "            for point in poses[0].body.keypoints:\n",
    "                if point is not None:\n",
    "                    xy.append([point.x, point.y])\n",
    "                else:\n",
    "                    xy.append([0, 0])\n",
    "            return np.array(xy)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "# Compute openpose conditioning image.\n",
    "openpose = MyOpenPoseDetector.from_pretrained(\"lllyasviel/ControlNet\")\n",
    "import pickle\n",
    "poses = []\n",
    "\n",
    "left = 30000\n",
    "right = 50000\n",
    "\n",
    "for idx, i in enumerate(tqdm(df['fullpath'].values[left:right], total=len(df.iloc[left:right]))):\n",
    "    try:\n",
    "        image = load_image(i)\n",
    "        pose = openpose(image)\n",
    "    except Exception as e:\n",
    "        pose = None\n",
    "\n",
    "    if pose is None:\n",
    "        poses.append(pose)\n",
    "    else:\n",
    "        poses.append(pickle.dumps(pose))\n",
    "\n",
    "sliced = df.iloc[left:right]\n",
    "sliced['pose_512'] = poses\n",
    "sliced['pose_is_none'] = sliced['pose_512'].apply(lambda x: x is None)\n",
    "sliced.to_csv(f'/home/roman/tryondiffusion_implementation/tryondiffusion_danny/all_imgs_clean_80756_{left}-{right}.csv', index=False)\n",
    "\n",
    "# zzz = df['fullpath'].values[2]\n",
    "# image = load_image(zzz)\n",
    "# pose = openpose(image)\n",
    "# print(pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2095431/3637896508.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  gig['pose_512'] = poses\n",
      "/tmp/ipykernel_2095431/3637896508.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  gig['pose_is_none'] = gig['pose_512'].apply(lambda x: x is None)\n"
     ]
    }
   ],
   "source": [
    "gig = df.iloc[:5725]\n",
    "gig['pose_512'] = poses\n",
    "gig['pose_is_none'] = gig['pose_512'].apply(lambda x: x is None)\n",
    "gig.to_csv('/home/roman/tryondiffusion_implementation/tryondiffusion_danny/all_imgs_clean_80756_5725.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[1. 2. 3. 4. 5. 6.]\n"
     ]
    }
   ],
   "source": [
    "array = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "s = np.array2string(array)\n",
    "print(s)\n",
    "reconstructed_array = np.fromstring(s.replace('[','').replace(']','').replace('\\n',''), sep=' ')\n",
    "print(reconstructed_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x80\\x04\\x95\\xa8\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x15numpy.core.multiarray\\x94\\x8c\\x0c_reconstruct\\x94\\x93\\x94\\x8c\\x05numpy\\x94\\x8c\\x07ndarray\\x94\\x93\\x94K\\x00\\x85\\x94C\\x01b\\x94\\x87\\x94R\\x94(K\\x01K\\x04\\x85\\x94h\\x03\\x8c\\x05dtype\\x94\\x93\\x94\\x8c\\x02f8\\x94\\x89\\x88\\x87\\x94R\\x94(K\\x03\\x8c\\x01<\\x94NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00t\\x94b\\x89C \\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x00\\x00\\x00\\x00\\x00\\x00\\xf0?\\x94t\\x94b.'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "a = np.ones(4)\n",
    "# string = pickle.dumps(a)\n",
    "print(string)\n",
    "pickle.loads(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[0.505859375, 0.146875], [0.462890625, 0.2359...\n",
       "1       [[0.54296875, 0.2234375], [0.55859375, 0.36406...\n",
       "2                                                    None\n",
       "3       [[0.328125, 0.2171875], [0.529296875, 0.285937...\n",
       "4       [[0.703125, 0.1359375], [0.544921875, 0.314062...\n",
       "                              ...                        \n",
       "2431    [[0.47265625, 0.125], [0.525390625, 0.2203125]...\n",
       "2432    [[0.380859375, 0.0], [0.234375, 0.3421875], [0...\n",
       "2433    [[0.494140625, 0.0], [0.52734375, 0.1890625], ...\n",
       "2434    [[0.0, 0.0], [0.56640625, 0.1609375], [0.80468...\n",
       "2435    [[0.494140625, 0.0], [0.505859375, 0.1015625],...\n",
       "Name: pose_512, Length: 2436, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pose_512']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pose_is_none'] = df['pose_512'].apply(lambda x: x is None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/roman/tryondiffusion_implementation/tryondiffusion_danny/all_imgs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = df[df['pose_is_none']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# print(len(xxx))\n",
    "# for idx, (ridx, row) in enumerate(xxx.iterrows()):\n",
    "#     if idx > 100:\n",
    "#         print(row['fullpath'])\n",
    "#         img = load_image(row['fullpath'])\n",
    "#         plt.imshow(img)\n",
    "#         plt.show()\n",
    "#     if idx > 150:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/roman/tryondiffusion_implementation/tryondiffusion_danny/nb.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btryon/home/roman/tryondiffusion_implementation/tryondiffusion_danny/nb.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m np\u001b[39m.\u001b[39;49mfrombuffer(df\u001b[39m.\u001b[39;49miloc[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mpose_512\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "np.(df.iloc[0]['pose_512'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/venv_tryondiffusion_implementation/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[28], line 1\u001b[0;36m\n\u001b[0;31m    eval(df.iloc[0]['pose_512'].replace('\\n', ','))\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    [[0.50585938 0.146875  ], [0.46289062 0.2359375 ], [0.37304688 0.24375   ], [0.33789062 0.3546875 ], [0.40820312 0.375     ], [0.55078125 0.225     ], [0.5859375  0.3296875 ], [0.50195312 0.3734375 ], [0.41992188 0.4546875 ], [0.42773438 0.6171875 ], [0.45117188 0.796875  ], [0.53320312 0.45      ], [0.55273438 0.6234375 ], [0.55273438 0.825     ], [0.48046875 0.1359375 ], [0.5078125  0.1296875 ], [0.42578125 0.1453125 ], [0.         0.        ]]\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "df.iloc[0]['pose_512'].replace('\\n', ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "pose = openpose(load_image('/mnt/datadrive/asos_dataset/tshirts_orig_bigest/111/54.png'))\n",
    "print(pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullpath</th>\n",
       "      <th>item_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            fullpath  item_idx\n",
       "0  /mnt/datadrive/asos_dataset/tshirts_orig_biges...         0\n",
       "1  /mnt/datadrive/asos_dataset/tshirts_orig_biges...         0\n",
       "2  /mnt/datadrive/asos_dataset/tshirts_orig_biges...         0\n",
       "3  /mnt/datadrive/asos_dataset/tshirts_orig_biges...         0\n",
       "4  /mnt/datadrive/asos_dataset/tshirts_orig_biges...         1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/roman/tryondiffusion_implementation/tryondiffusion_danny/all_imgs.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2436/2436 [14:11<00:00,  2.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "new_boba = []\n",
    "for i in tqdm(df['fullpath'].values, total=len(df)):\n",
    "    res = openpose(load_image(i))\n",
    "    # print(res)\n",
    "    # if res is not None:\n",
    "        # print(res.shape)\n",
    "    new_boba.append(res is not None)\n",
    "    \n",
    "\n",
    "df['pose_detected'] = new_boba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/home/roman/tryondiffusion_implementation/tryondiffusion_danny/all_imgs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/roman/tryondiffusion_implementation/tryondiffusion_danny/all_imgs.csv')\n",
    "# items_reverse_index = {}\n",
    "# df = df[df['pose_detected']]\n",
    "# for group_idx, group in df.groupby(by='item_idx'):    \n",
    "#     items_reverse_index[group_idx] = group['fullpath'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fullpath</th>\n",
       "      <th>item_idx</th>\n",
       "      <th>pose_detected</th>\n",
       "      <th>pose_512</th>\n",
       "      <th>pose_is_none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>b'\\x80\\x04\\x95\\xad\\x01\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>b'\\x80\\x04\\x95\\xad\\x01\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>b'\\x80\\x04\\x95\\xad\\x01\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>b'\\x80\\x04\\x95\\xad\\x01\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "      <td>97</td>\n",
       "      <td>True</td>\n",
       "      <td>b'\\x80\\x04\\x95\\xad\\x01\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "      <td>99</td>\n",
       "      <td>True</td>\n",
       "      <td>b'\\x80\\x04\\x95\\xad\\x01\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2433</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "      <td>99</td>\n",
       "      <td>True</td>\n",
       "      <td>b'\\x80\\x04\\x95\\xad\\x01\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "      <td>99</td>\n",
       "      <td>True</td>\n",
       "      <td>b'\\x80\\x04\\x95\\xad\\x01\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2435</th>\n",
       "      <td>/mnt/datadrive/asos_dataset/tshirts_orig_biges...</td>\n",
       "      <td>99</td>\n",
       "      <td>True</td>\n",
       "      <td>b'\\x80\\x04\\x95\\xad\\x01\\x00\\x00\\x00\\x00\\x00\\x00...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2436 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               fullpath  item_idx  \\\n",
       "0     /mnt/datadrive/asos_dataset/tshirts_orig_biges...         0   \n",
       "1     /mnt/datadrive/asos_dataset/tshirts_orig_biges...         0   \n",
       "2     /mnt/datadrive/asos_dataset/tshirts_orig_biges...         0   \n",
       "3     /mnt/datadrive/asos_dataset/tshirts_orig_biges...         0   \n",
       "4     /mnt/datadrive/asos_dataset/tshirts_orig_biges...         1   \n",
       "...                                                 ...       ...   \n",
       "2431  /mnt/datadrive/asos_dataset/tshirts_orig_biges...        97   \n",
       "2432  /mnt/datadrive/asos_dataset/tshirts_orig_biges...        99   \n",
       "2433  /mnt/datadrive/asos_dataset/tshirts_orig_biges...        99   \n",
       "2434  /mnt/datadrive/asos_dataset/tshirts_orig_biges...        99   \n",
       "2435  /mnt/datadrive/asos_dataset/tshirts_orig_biges...        99   \n",
       "\n",
       "      pose_detected                                           pose_512  \\\n",
       "0              True  b'\\x80\\x04\\x95\\xad\\x01\\x00\\x00\\x00\\x00\\x00\\x00...   \n",
       "1              True  b'\\x80\\x04\\x95\\xad\\x01\\x00\\x00\\x00\\x00\\x00\\x00...   \n",
       "2             False                                                NaN   \n",
       "3              True  b'\\x80\\x04\\x95\\xad\\x01\\x00\\x00\\x00\\x00\\x00\\x00...   \n",
       "4              True  b'\\x80\\x04\\x95\\xad\\x01\\x00\\x00\\x00\\x00\\x00\\x00...   \n",
       "...             ...                                                ...   \n",
       "2431           True  b'\\x80\\x04\\x95\\xad\\x01\\x00\\x00\\x00\\x00\\x00\\x00...   \n",
       "2432           True  b'\\x80\\x04\\x95\\xad\\x01\\x00\\x00\\x00\\x00\\x00\\x00...   \n",
       "2433           True  b'\\x80\\x04\\x95\\xad\\x01\\x00\\x00\\x00\\x00\\x00\\x00...   \n",
       "2434           True  b'\\x80\\x04\\x95\\xad\\x01\\x00\\x00\\x00\\x00\\x00\\x00...   \n",
       "2435           True  b'\\x80\\x04\\x95\\xad\\x01\\x00\\x00\\x00\\x00\\x00\\x00...   \n",
       "\n",
       "      pose_is_none  \n",
       "0            False  \n",
       "1            False  \n",
       "2             True  \n",
       "3            False  \n",
       "4            False  \n",
       "...            ...  \n",
       "2431         False  \n",
       "2432         False  \n",
       "2433         False  \n",
       "2434         False  \n",
       "2435         False  \n",
       "\n",
       "[2436 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x80\\x04\\x95\\xad\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x15numpy.core.multiarray\\x94\\x8c\\x0c_reconstruct\\x94\\x93\\x94\\x8c\\x05numpy\\x94\\x8c\\x07ndarray\\x94\\x93\\x94K\\x00\\x85\\x94C\\x01b\\x94\\x87\\x94R\\x94(K\\x01K\\x12K\\x02\\x86\\x94h\\x03\\x8c\\x05dtype\\x94\\x93\\x94\\x8c\\x02f8\\x94\\x89\\x88\\x87\\x94R\\x94(K\\x03\\x8c\\x01<\\x94NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00t\\x94b\\x89B \\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x000\\xe0?\\xcd\\xcc\\xcc\\xcc\\xcc\\xcc\\xc2?\\x00\\x00\\x00\\x00\\x00\\xa0\\xdd?333333\\xce?\\x00\\x00\\x00\\x00\\x00\\xe0\\xd7?333333\\xcf?\\x00\\x00\\x00\\x00\\x00\\xa0\\xd5?33333\\xb3\\xd6?\\x00\\x00\\x00\\x00\\x00 \\xda?\\x00\\x00\\x00\\x00\\x00\\x00\\xd8?\\x00\\x00\\x00\\x00\\x00\\xa0\\xe1?\\xcd\\xcc\\xcc\\xcc\\xcc\\xcc\\xcc?\\x00\\x00\\x00\\x00\\x00\\xc0\\xe2?\\x9a\\x99\\x99\\x99\\x99\\x19\\xd5?\\x00\\x00\\x00\\x00\\x00\\x10\\xe0?fffff\\xe6\\xd7?\\x00\\x00\\x00\\x00\\x00\\xe0\\xda?\\x9a\\x99\\x99\\x99\\x99\\x19\\xdd?\\x00\\x00\\x00\\x00\\x00`\\xdb?\\x00\\x00\\x00\\x00\\x00\\xc0\\xe3?\\x00\\x00\\x00\\x00\\x00\\xe0\\xdc?\\x00\\x00\\x00\\x00\\x00\\x80\\xe9?\\x00\\x00\\x00\\x00\\x00\\x10\\xe1?\\xcd\\xcc\\xcc\\xcc\\xcc\\xcc\\xdc?\\x00\\x00\\x00\\x00\\x00\\xb0\\xe1?33333\\xf3\\xe3?\\x00\\x00\\x00\\x00\\x00\\xb0\\xe1?ffffff\\xea?\\x00\\x00\\x00\\x00\\x00\\xc0\\xde?ffffff\\xc1?\\x00\\x00\\x00\\x00\\x00@\\xe0?\\x9a\\x99\\x99\\x99\\x99\\x99\\xc0?\\x00\\x00\\x00\\x00\\x00@\\xdb?\\x9a\\x99\\x99\\x99\\x99\\x99\\xc2?\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x94t\\x94b.'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "prepared_ds_path = Path('/mnt/datadrive/asos_dataset/prepared_64/tensors').rglob('*.pt')\n",
    "for i in prepared_ds_path:\n",
    "    xx = torch.load(i)\n",
    "    print(xx['person_poses'])   \n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tryondiffusion_implementation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
