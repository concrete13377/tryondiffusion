{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/datadrive/asos_dataset/prepared_64/tensors/0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/604 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from clothing_segmentation import HumanParser\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from torchvision import transforms as T\n",
    "\n",
    "\n",
    "\n",
    "class SyntheticTryonDatasetFromDisk(Dataset):\n",
    "    def __init__(self, path='/mnt/datadrive/asos_dataset/prepared_256/tensors/'):\n",
    "        self.path = Path(path)\n",
    "        self.glob=sorted(self.path.rglob('*.pt'), key=lambda x: int(x.stem))\n",
    "        print(self.glob[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.glob)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tensor_path = self.glob[idx]\n",
    "        record = torch.load(tensor_path)\n",
    "        return record\n",
    "\n",
    "\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "xxxx = SyntheticTryonDatasetFromDisk()\n",
    "bad_list=[]\n",
    "from tqdm import tqdm\n",
    "for idx, i in enumerate(tqdm(xxxx, total=len(xxxx))):\n",
    "    # print(i)\n",
    "    print(i['person_poses'].shape)\n",
    "    \n",
    "    \n",
    "    # print(i['garment_poses'])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def __call__(self, input_image, detect_resolution=512, include_hand=False, include_face=False):\n",
    "    \n",
    "#     if not isinstance(input_image, np.ndarray):\n",
    "#         input_image = np.array(input_image, dtype=np.uint8)\n",
    "\n",
    "#     input_image = HWC3(input_image)\n",
    "#     input_image = resize_image(input_image, detect_resolution)\n",
    "\n",
    "#     poses = self.detect_poses(input_image, include_hand, include_face)\n",
    "    \n",
    "#     xy=[]\n",
    "#     if len(poses)>0:\n",
    "#         for point in poses[0].body.keypoints:\n",
    "#             if point is not None:\n",
    "#                 xy.append([point.x, point.y])\n",
    "#             else:\n",
    "#                 xy.append([0, 0])\n",
    "#         return np.array(xy)\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "from controlnet_aux import OpenposeDetector\n",
    "from controlnet_aux.open_pose import HWC3, resize_image\n",
    "from diffusers.utils import load_image\n",
    "input_image = target_img\n",
    "if not isinstance(input_image, np.ndarray):\n",
    "    input_image = np.array(input_image, dtype=np.uint8)\n",
    "input_image = input_image.astype(np.uint8)\n",
    "\n",
    "input_image = HWC3(input_image)\n",
    "input_image = resize_image(input_image, 512)\n",
    "\n",
    "poses = dataset.openpose.detect_poses(input_image, include_hand=False, include_face=False)\n",
    "\n",
    "# xy=[]\n",
    "# if len(poses)>0:\n",
    "#     for point in poses[0].body.keypoints:\n",
    "#         if point is not None:\n",
    "#             xy.append([point.x, point.y])\n",
    "#         else:\n",
    "#             xy.append([0, 0])\n",
    "#     return np.array(xy)\n",
    "# else:\n",
    "#     return None\n",
    "poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.9882353 , 0.9882353 , 0.9882353 ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        ...,\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ]],\n",
       "\n",
       "       [[1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        ...,\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ]],\n",
       "\n",
       "       [[1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        ...,\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        ...,\n",
       "        [0.99215686, 0.99215686, 0.99215686],\n",
       "        [0.99215686, 0.99215686, 0.99215686],\n",
       "        [0.99215686, 0.99215686, 0.99215686]],\n",
       "\n",
       "       [[1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        ...,\n",
       "        [0.9882353 , 0.9882353 , 0.9882353 ],\n",
       "        [0.9882353 , 0.9882353 , 0.9882353 ],\n",
       "        [0.9882353 , 0.9882353 , 0.9882353 ]],\n",
       "\n",
       "       [[1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        ...,\n",
       "        [0.9764706 , 0.9764706 , 0.9764706 ],\n",
       "        [0.9764706 , 0.9764706 , 0.9764706 ],\n",
       "        [0.9764706 , 0.9764706 , 0.9764706 ]]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "target_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/mnt/datadrive/asos_dataset/tshirts_orig_bigest/65/2288.png',\n",
       "       '/mnt/datadrive/asos_dataset/tshirts_orig_bigest/65/2289.png',\n",
       "       '/mnt/datadrive/asos_dataset/tshirts_orig_bigest/65/2290.png'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.items_reverse_index[65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[65,\n",
       " 109,\n",
       " 128,\n",
       " 135,\n",
       " 136,\n",
       " 179,\n",
       " 212,\n",
       " 227,\n",
       " 232,\n",
       " 250,\n",
       " 269,\n",
       " 318,\n",
       " 361,\n",
       " 440,\n",
       " 455,\n",
       " 465,\n",
       " 466,\n",
       " 495,\n",
       " 531,\n",
       " 534,\n",
       " 535,\n",
       " 545,\n",
       " 568]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roman/venv_tryondiffusion_implementation/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/roman/venv_tryondiffusion_implementation/lib/python3.10/site-packages/controlnet_aux/mediapipe_face/mediapipe_face_common.py:7: UserWarning: The module 'mediapipe' is not installed. The package will have limited functionality. Please install it using the command: pip install 'mediapipe'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80761\n",
      "80106\n",
      "203404\n"
     ]
    }
   ],
   "source": [
    "# import pickle \n",
    "\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# from pathlib import Path\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# from clothing_segmentation import HumanParser\n",
    "# import pandas as pd\n",
    "\n",
    "# from torchvision import transforms as T\n",
    "\n",
    "\n",
    "# from controlnet_aux import OpenposeDetector\n",
    "# from controlnet_aux.open_pose import HWC3, resize_image\n",
    "# from diffusers.utils import load_image\n",
    "\n",
    "\n",
    "\n",
    "# class MyOpenPoseDetector(OpenposeDetector):\n",
    "    \n",
    "#     def __call__(self, input_image, detect_resolution=512, include_hand=False, include_face=False):\n",
    "       \n",
    "#         if not isinstance(input_image, np.ndarray):\n",
    "#             input_image = np.array(input_image, dtype=np.uint8)\n",
    "\n",
    "#         input_image = HWC3(input_image)\n",
    "#         input_image = resize_image(input_image, detect_resolution)\n",
    "\n",
    "#         poses = self.detect_poses(input_image, include_hand, include_face)\n",
    "#         xy=[]\n",
    "#         if len(poses)>0:\n",
    "#             for point in poses[0].body.keypoints:\n",
    "#                 if point is not None:\n",
    "#                     xy.append([point.x, point.y])\n",
    "#                 else:\n",
    "#                     xy.append([0, 0])\n",
    "#             return np.array(xy)\n",
    "#         else:\n",
    "#             return None\n",
    "        \n",
    "\n",
    "# import itertools\n",
    "# class SyntheticTryonDataset(Dataset):\n",
    "#     def __init__(self, image_size=(64,64), pose_size=(18, 2), apply_transform=True):\n",
    "#         self.cache = {}\n",
    "#         self.human_parser = HumanParser()\n",
    "#         self.transform = T.Compose([\n",
    "#             # T.Resize(image_size),\n",
    "#             # T.CenterCrop(image_size),\n",
    "#             T.ToTensor(),\n",
    "#         ])\n",
    "#         self.apply_transform = apply_transform\n",
    "#         # self.df = pd.read_csv('/home/roman/tryondiffusion_implementation/tryondiffusion_danny/all_imgs.csv')\n",
    "#         self.df = pd.read_csv('/home/roman/tryondiffusion_implementation/tryondiffusion_danny/all_imgs_clean_80756_total.csv')\n",
    "#         print(len(self.df))\n",
    "#         self.df = self.df[~self.df['pose_is_none']]\n",
    "#         print(len(self.df))\n",
    "#         self.items_reverse_index = {}\n",
    "#         self.items_reverse_index_poses = {}\n",
    "#         for group_idx, group in self.df.groupby(by='item_idx'):\n",
    "#             self.items_reverse_index[group_idx] = [{\"fullpath\":fp, \"pose_512\":pose} for fp, pose in zip(group['fullpath'].values, group['pose_512'].values)]        \n",
    "\n",
    "#         self.items_reverse_index3 = {}\n",
    "#         key_idx=0\n",
    "#         for k,v in self.items_reverse_index.items():\n",
    "#             permutations = list(itertools.permutations(v, 2))\n",
    "#             for i in permutations:\n",
    "#                 self.items_reverse_index3[key_idx] = i\n",
    "#                 key_idx+=1\n",
    "\n",
    "#         print(len(self.items_reverse_index3))\n",
    "#         self.image_size = image_size\n",
    "#         # self.pose_size = pose_size\n",
    "#         self.openpose = MyOpenPoseDetector.from_pretrained(\"lllyasviel/ControlNet\")\n",
    "\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.items_reverse_index3)\n",
    "    \n",
    "#     def prepare_clothing_agnostic(self, img, hp_mask)-> np.array: \n",
    "#         # classes_to_rm=[4,6] \n",
    "#         classes_to_rm=[4]      \n",
    "#         # def get_clothing_agnostic(image, hp_mask, classes_to_rm=[4,6]):\n",
    "#         bg_color = (255,255,255)\n",
    "#         assert img.shape[:-1] == hp_mask.shape\n",
    "#         # cloths_to_rm_mask = np.zeros(hp_mask.shape)\n",
    "#         # for i in np.unique(res):\n",
    "#         #     if i in classes_to_rm:\n",
    "#         #         cloths_to_rm_mask[res==i] = 255\n",
    "#         cloths_to_rm_mask = np.isin(hp_mask, classes_to_rm)\n",
    "#         img[cloths_to_rm_mask!=0] = bg_color\n",
    "#         return img\n",
    "        \n",
    "#     def prepare_segmented_garment(self, img, hp_mask)-> np.array:\n",
    "#         # classes_to_rm=[4,6] \n",
    "#         classes_to_rm=[4]        \n",
    "#         bg_color = (255,255,255)\n",
    "#         assert img.shape[:-1] == hp_mask.shape\n",
    "#         # cloths_to_rm_mask = np.zeros(hp_mask.shape)\n",
    "#         # for i in np.unique(res):\n",
    "#         #     if i in classes_to_rm:\n",
    "#         #         cloths_to_rm_mask[res==i] = 255\n",
    "#         cloths_to_rm_mask = np.isin(hp_mask, classes_to_rm)\n",
    "#         img[cloths_to_rm_mask==0] = bg_color\n",
    "#         return img\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = self.items_reverse_index3[idx]\n",
    "#         img_person = item[0]['fullpath']\n",
    "#         person_dict = None\n",
    "#         if img_person in self.cache:\n",
    "#             person_dict = self.cache[img_person]\n",
    "#         # person_pose = item[0]['pose_512']\n",
    "#         # person_pose = pickle.loads(person_pose)\n",
    "\n",
    "#         img_garment = item[1]['fullpath']\n",
    "#         garment_dict = None\n",
    "#         if img_garment in self.cache:\n",
    "#             garment_dict = self.cache[img_garment]\n",
    "        \n",
    "#         if person_dict is None:\n",
    "#             person_image = Image.open(img_person).convert('RGB').resize((768, 768), Image.BICUBIC)\n",
    "#             op_img = load_image(img_person)\n",
    "#             person_pose = self.openpose(op_img)\n",
    "            \n",
    "#             np_person_image = np.array(person_image)\n",
    "#             person_image_resized = person_image.resize(self.image_size, Image.BICUBIC)\n",
    "#             person_image_hp = self.human_parser.forward_img(person_image).squeeze(0)\n",
    "            \n",
    "#             ca_image = self.prepare_clothing_agnostic(np_person_image, person_image_hp)\n",
    "\n",
    "#             person_dict = {\n",
    "#                 \"person_images\": self.transform(person_image_resized),\n",
    "#                 \"ca_images\": self.transform(Image.fromarray(ca_image.astype('uint8')).resize(self.image_size, Image.BICUBIC)),\n",
    "#                 \"person_poses\": person_pose\n",
    "#             }\n",
    "#             self.cache[img_person] = person_dict\n",
    "        \n",
    "#         if garment_dict is None:\n",
    "#             garment_image = Image.open(img_garment).convert('RGB').resize((768, 768), Image.BICUBIC)\n",
    "#             op_img = load_image(img_garment)\n",
    "#             garment_pose  = self.openpose(op_img)\n",
    "            \n",
    "#             np_garment_image = np.array(garment_image)\n",
    "#             garment_image_hp = self.human_parser.forward_img(garment_image).squeeze(0)\n",
    "            \n",
    "#             segmented_garment = self.prepare_segmented_garment(np_garment_image, garment_image_hp) \n",
    "\n",
    "#             garment_dict = {\n",
    "#                 \"garment_images\": self.transform(Image.fromarray(segmented_garment.astype('uint8')).resize(self.image_size, Image.BICUBIC)),\n",
    "#                 \"garment_poses\": garment_pose    \n",
    "#             }\n",
    "#             self.cache[img_garment] = garment_dict\n",
    "\n",
    "\n",
    "\n",
    "#         # sample = {\n",
    "#             # \"person_images\": person_image_resized,\n",
    "#             # \"ca_images\": Image.fromarray(ca_image.astype('uint8')).resize(self.image_size, Image.BICUBIC),\n",
    "#             # \"garment_images\": Image.fromarray(segmented_garment.astype('uint8')).resize(self.image_size, Image.BICUBIC),\n",
    "#             # \"person_poses\": person_pose,\n",
    "#             # \"garment_poses\": garment_pose,\n",
    "#         # }\n",
    "#         sample = {\n",
    "#             **person_dict, **garment_dict\n",
    "#         }\n",
    "        \n",
    "#         # if self.apply_transform:\n",
    "#         #     sample = {\n",
    "#         #         \"person_images\": self.transform(sample['person_images']),\n",
    "#         #         \"ca_images\": self.transform(sample['ca_images']),\n",
    "#         #         \"garment_images\": self.transform(sample['garment_images']),\n",
    "#         #         \"person_poses\": sample['person_poses'],\n",
    "#         #         \"garment_poses\": sample['garment_poses']\n",
    "#         #     }\n",
    "        \n",
    "#         return sample\n",
    "    \n",
    "\n",
    "# class SyntheticTryonDatasetFromDisk(Dataset):\n",
    "#     # def __init__(self, max_imgs, path='/mnt/datadrive/asos_dataset/prepared_256/tensors/'):\n",
    "#     def __init__(self, max_imgs=None, path='/mnt/datadrive/dress_code/prepared_128/tensors/'):\n",
    "#         self.path = Path(path)\n",
    "#         self.glob=sorted(self.path.rglob('*.pt'), key=lambda x: int(x.stem))\n",
    "#         if max_imgs is not None:\n",
    "#             self.glob = self.glob[:max_imgs]\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.glob)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         tensor_path = self.glob[idx]\n",
    "#         record = torch.load(tensor_path)\n",
    "#         newrecord = {}\n",
    "#         for k,v in record.items():\n",
    "#             if 'pose' in k:\n",
    "#                 newrecord[k] = torch.tensor(v, dtype=torch.float)\n",
    "#             else: \n",
    "#                 newrecord[k]=v\n",
    "#         # for k,v in newrecord.items():\n",
    "#         #     print(k, type(v), v.dtype)\n",
    "#         return newrecord\n",
    "    \n",
    "\n",
    "# def tryondiffusion_collate_fn(batch):\n",
    "#     return {\n",
    "#         \"person_images\": torch.stack([item[\"person_images\"] for item in batch]),\n",
    "#         \"ca_images\": torch.stack([item[\"ca_images\"] for item in batch]),\n",
    "#         \"garment_images\": torch.stack([item[\"garment_images\"] for item in batch]),\n",
    "#         \"person_poses\": torch.stack([item[\"person_poses\"] for item in batch]),\n",
    "#         \"garment_poses\": torch.stack([item[\"garment_poses\"] for item in batch]),\n",
    "#     }\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append('/home/roman/tryondiffusion_implementation/tryondiffusion_danny/tryondiffusion')\n",
    "from tryondiffusion import TryOnImagen, TryOnImagenTrainer, get_unet_by_name, SyntheticTryonDataset, tryondiffusion_collate_fn\n",
    "\n",
    "\n",
    "# TODO resize pose from 512 to 64\n",
    "\n",
    "\n",
    "TRAIN_UNET_NUMBER = 1\n",
    "BASE_UNET_IMAGE_SIZE = (128, 128)\n",
    "SR_UNET_IMAGE_SIZE = (256, 256)\n",
    "BATCH_SIZE = 2 \n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "NUM_ITERATIONS = 100\n",
    "TIMESTEPS = (1000, 1000)\n",
    "\n",
    "dataset = SyntheticTryonDataset(image_size=SR_UNET_IMAGE_SIZE if TRAIN_UNET_NUMBER == 2 else BASE_UNET_IMAGE_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 102/203404 [03:35<119:34:29,  2.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/roman/tryondiffusion_implementation/tryondiffusion_danny/1_prepare_data_on_disk.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btryon/home/roman/tryondiffusion_implementation/tryondiffusion_danny/1_prepare_data_on_disk.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m trange(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(dataset)):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btryon/home/roman/tryondiffusion_implementation/tryondiffusion_danny/1_prepare_data_on_disk.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btryon/home/roman/tryondiffusion_implementation/tryondiffusion_danny/1_prepare_data_on_disk.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         sample \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(idx)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btryon/home/roman/tryondiffusion_implementation/tryondiffusion_danny/1_prepare_data_on_disk.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         torch\u001b[39m.\u001b[39msave(sample, prepared_ds_path \u001b[39m/\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btryon/home/roman/tryondiffusion_implementation/tryondiffusion_danny/1_prepare_data_on_disk.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/tryondiffusion_implementation/tryondiffusion_danny/tryondiffusion/dataset.py:166\u001b[0m, in \u001b[0;36mSyntheticTryonDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39m# print(garment_image.size)\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39m# .resize(self.image_size, Image.BICUBIC)\u001b[39;00m\n\u001b[1;32m    165\u001b[0m np_garment_image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(garment_image)\n\u001b[0;32m--> 166\u001b[0m garment_image_hp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhuman_parser\u001b[39m.\u001b[39;49mforward_img(garment_image)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    168\u001b[0m segmented_garment \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_segmented_garment(np_garment_image, garment_image_hp) \n\u001b[1;32m    170\u001b[0m \u001b[39m# person_image = torch.randn(3, *self.image_size)\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# ca_image = torch.randn(3, *self.image_size)\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# garment_image = torch.randn(3, *self.image_size)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39m#     \"garment_poses\": garment_pose,\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39m# }\u001b[39;00m\n",
      "File \u001b[0;32m~/tryondiffusion_implementation/tryondiffusion_danny/clothing_segmentation/human_parsing.py:26\u001b[0m, in \u001b[0;36mHumanParser.forward_img\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     24\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_processor(images\u001b[39m=\u001b[39mimage, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 26\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     27\u001b[0m     logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits  \u001b[39m# shape (batch_size, num_labels, height/4, width/4)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m# print(f'{logits.shape=}')\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m# print(f'{image.size}')\u001b[39;00m\n",
      "File \u001b[0;32m~/venv_tryondiffusion_implementation/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/venv_tryondiffusion_implementation/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv_tryondiffusion_implementation/lib/python3.10/site-packages/transformers/models/segformer/modeling_segformer.py:802\u001b[0m, in \u001b[0;36mSegformerForSemanticSegmentation.forward\u001b[0;34m(self, pixel_values, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    793\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msegformer(\n\u001b[1;32m    794\u001b[0m     pixel_values,\n\u001b[1;32m    795\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    796\u001b[0m     output_hidden_states\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,  \u001b[39m# we need the intermediate hidden states\u001b[39;00m\n\u001b[1;32m    797\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    798\u001b[0m )\n\u001b[1;32m    800\u001b[0m encoder_hidden_states \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mhidden_states \u001b[39mif\u001b[39;00m return_dict \u001b[39melse\u001b[39;00m outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 802\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecode_head(encoder_hidden_states)\n\u001b[1;32m    804\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    805\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    806\u001b[0m     \u001b[39m# upsample logits to the images' original size\u001b[39;00m\n",
      "File \u001b[0;32m~/venv_tryondiffusion_implementation/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/venv_tryondiffusion_implementation/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv_tryondiffusion_implementation/lib/python3.10/site-packages/transformers/models/segformer/modeling_segformer.py:728\u001b[0m, in \u001b[0;36mSegformerDecodeHead.forward\u001b[0;34m(self, encoder_hidden_states)\u001b[0m\n\u001b[1;32m    723\u001b[0m     encoder_hidden_state \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39minterpolate(\n\u001b[1;32m    724\u001b[0m         encoder_hidden_state, size\u001b[39m=\u001b[39mencoder_hidden_states[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize()[\u001b[39m2\u001b[39m:], mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m\"\u001b[39m, align_corners\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    725\u001b[0m     )\n\u001b[1;32m    726\u001b[0m     all_hidden_states \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (encoder_hidden_state,)\n\u001b[0;32m--> 728\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear_fuse(torch\u001b[39m.\u001b[39;49mcat(all_hidden_states[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m    729\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_norm(hidden_states)\n\u001b[1;32m    730\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(hidden_states)\n",
      "File \u001b[0;32m~/venv_tryondiffusion_implementation/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/venv_tryondiffusion_implementation/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv_tryondiffusion_implementation/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/venv_tryondiffusion_implementation/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prepared_ds_path = Path('/mnt/datadrive/asos_dataset/80756_203404/prepared_128/tensors')\n",
    "prepared_ds_path.mkdir(parents=True, exist_ok=True)\n",
    "# for idx, sample in enumerate(tqdm(dataset, total=len(dataset))):\n",
    "#     if idx > 103:\n",
    "#         # print(sample.keys())\n",
    "#         # for k,v in sample.items():\n",
    "#         #     print(v.shape)\n",
    "#         try:\n",
    "#             torch.save(sample, prepared_ds_path / f'{idx}.pt')\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             print(idx)\n",
    "\n",
    "from tqdm import trange\n",
    "for idx in trange(0, len(dataset)):\n",
    "    try:\n",
    "        sample = dataset.__getitem__(idx)\n",
    "        torch.save(sample, prepared_ds_path / f'{idx}.pt')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tryondiffusion_implementation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
